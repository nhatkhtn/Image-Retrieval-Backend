{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153bda5b-303f-4a5e-9607-b306887b8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef206089-dd81-45ca-b0f2-e1a026962f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional import average_precision\n",
    "# \n",
    "from network import Model \n",
    "from datasets.video import VideoDataset\n",
    "from datasets.description import DescriptionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7824ccaa-5faf-40ba-86e5-1d3ddfe92270",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb84c858-f968-417b-8e7e-5995ac895c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e30bf5-cf6f-45a5-a92d-a43979cc8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 out of 500 videos accepted in /home/nero/Courses/CS412/datasets/TRECVid Data/testing_set/Frames.\n",
      "1790 descriptions of 500 videos accepted in /home/nero/Courses/CS412/datasets/TRECVid Data/testing_set.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0853a695-ee0e-40d2-a5c6-00f1bd5ba0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00004', '00007', '00015', '00041', '00047', '00063', '00067', '00084', '00089', '00100', '00113', '00129', '00145', '00175', '00194', '00197', '00219', '00229', '00236', '00239', '00243', '00248', '00342', '00361', '00389', '00392', '00412', '00440', '00445', '00459', '00492', '00533', '00534', '00535', '00543', '00545', '00554', '00562', '00583', '00591']\n",
      "500 out of 500 videos accepted in ../../../images/TRECVid Data/testing_set/Frames.\n",
      "1790 descriptions of 500 videos accepted in ../../../images/TRECVid Data/testing_set.\n"
     ]
    }
   ],
   "source": [
    "from datasets.description import DescriptionDataset\n",
    "from datasets.video_with_desc import VideoWithDescDataset\n",
    "videoDataset = VideoWithDescDataset(root = \"../../../images/TRECVid Data/testing_set\", num_imgs = 8, subset = 'test', preprocess = model.preprocess)\n",
    "descDataset = DescriptionDataset(root = \"../../../images/TRECVid Data/testing_set\", subset = 'test')\n",
    "\n",
    "videoDataloader = DataLoader(videoDataset, batch_size = 8, shuffle = False, num_workers = 8)\n",
    "descDataloader = DataLoader(descDataset, batch_size =8, shuffle = False, num_workers = 8)\n",
    "\n",
    "def getVideoFeatures(model, dataloader):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            images = data['images'].to(device)           # B, T, C, H, W \n",
    "            vid_ids = data['video_id']   # B \n",
    "\n",
    "            # IMAGES AS VIDEO \n",
    "            num_imgs = data['num_imgs'][0]\n",
    "            # print(images.shape)\n",
    "            images = images.unsqueeze(2).repeat(1, 1, 3, 1, 1, 1)\n",
    "            images[:, 2:, 0] = images[:, :-2, 2]\n",
    "            images[:, 1:, 0] = images[:, :-1, 2]\n",
    "            images = images.flatten(start_dim = 0, end_dim = 1) \n",
    "            # print(images.shape)\n",
    "            features = model.encode_images(images)\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(vid_ids.unsqueeze(-1).repeat(1, num_imgs).view(-1)) \n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "def getTextFeatures(model, dataloader):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            description = data['description'].to(device)\n",
    "            vid_ids = data[\"video_id\"]\n",
    "            features = model.encode_text(description)\n",
    "            \n",
    "            all_features.append(features)\n",
    "            all_labels.append(vid_ids) \n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "import time \n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.functional import *\n",
    "    \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def evaluate(model):\n",
    "    process_begin = time.time()\n",
    "    # Calculate the image features\n",
    "    videoFeatures, videoLabels = getVideoFeatures(model, videoDataloader)\n",
    "    textFeatures, textLabel = getTextFeatures(model, descDataloader)\n",
    "    target = textLabel.view(-1, 1) == videoLabels.view(1, -1)\n",
    "\n",
    "    logits_per_image, logits_per_text = model.calc_similarity(videoFeatures, textFeatures)\n",
    "\n",
    "    total_process_time = time.time() - process_begin\n",
    "    print(f\"Total process time: {total_process_time:03f}\")\n",
    "\n",
    "    mAP = torch.tensor([retrieval_average_precision(logits_per_text[i], target[i]) for i in range(logits_per_text.size(0))]).mean()\n",
    "    print(f\"Mean Average Precision: {mAP}\")\n",
    "    k = 15 \n",
    "    recK = torch.tensor([retrieval_recall(logits_per_text[i], target[i], k) for i in range(logits_per_text.size(0))]).mean()\n",
    "    print(f\"Mean Recall@{k}: {recK}\")\n",
    "    rr  = torch.tensor([retrieval_reciprocal_rank(logits_per_text[i], target[i]) for i in range(logits_per_text.size(0))]).mean()\n",
    "    print(f\"Mean Reciprocal Rank: {rr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9321b50-a5fb-4095-84f8-8bb72edd8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:15<00:00,  4.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 224/224 [00:02<00:00, 103.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total process time: 17.191969\n",
      "Mean Average Precision: 0.5955747365951538\n",
      "Mean Recall@15: 0.6545391082763672\n",
      "Mean Reciprocal Rank: 0.6981375217437744\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# from torch import nn\n",
    "print\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf65a3e-95b8-499a-877e-69ff95400f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436bfe88-b3fc-4621-ac8b-e14e3175b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image, logits_per_text = model.calc_similarity(videoFeatures, textFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a0a63b-8f2d-40d6-89cd-79d83e12c464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1790, 4000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0533db3-defe-47ec-9e30-95546c9bfa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = textLabel.view(-1, 1) == videoLabels.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0716fb36-31e3-4be6-971e-fa05bc72005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision: 0.39895907044410706\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional import *\n",
    "print(f\"Mean Average Precision: {retrieval_average_precision(logits_per_text, target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52da908e-2976-4296-a402-160de8d634fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank: 0.21305866539478302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Mean Reciprocal Rank: {retrieval_recall(logits_per_text, target, k = 4000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353d8915-0a40-453c-bd2a-b47d63d0a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "a = tensor([0.5, 0.2, 0.3])\n",
    "b = tensor([False, True ,False])\n",
    "retrieval_recall(a, b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4268e46-88bc-4c9a-9e53-ebfc90ed5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81eaf8eb-e3e9-46a8-b3f3-7b3874bad21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3adccd6-6ba8-4aab-98e6-80a343947968",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = 0\n",
    "rec15 = 0\n",
    "rank = 0\n",
    "for i in range(logits_per_text.size(0)):\n",
    "    # print(f\"AP - {i} - {textLabel[i]}: {retrieval_average_precision(logits_per_text[i], target[i])}\")\n",
    "    mAP += retrieval_average_precision(logits_per_text[i], target[i])\n",
    "    rec15 += retrieval_recall(logits_per_text[i], target[i], k = 15)\n",
    "    rank += retrieval_reciprocal_rank(logits_per_text[i], target[i])\n",
    "mAP /= logits_per_text.size(0)\n",
    "rec15 /= logits_per_text.size(0)\n",
    "rank /=logits_per_text.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d0723a-9afe-4f53-a1e6-c0ef64ba7160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5625) tensor(0.6269) tensor(0.6977)\n"
     ]
    }
   ],
   "source": [
    "print(mAP, rec15, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46a1664-3d9a-4a34-a4c9-d0997b2605bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_recall(logits_per_text[0], target[0], k = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340cf689-339b-4738-aef7-3ddb3a43ea6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchmetrics.functional.classification.precision_recall.recall(preds: torch.Tensor, target: torch.Tensor, average: str = 'micro', mdmc_average: Union[str, NoneType] = None, ignore_index: Union[int, NoneType] = None, num_classes: Union[int, NoneType] = None, threshold: float = 0.5, top_k: Union[int, NoneType] = None, multiclass: Union[bool, NoneType] = None) -> torch.Tensor>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9f979-6e7e-4cf6-a02b-8e55def0ca65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
